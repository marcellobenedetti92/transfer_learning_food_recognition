{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c67e10118d3b4a329f2967f07a7b0c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1749af1822647e09659a00318099947",
              "IPY_MODEL_ccafe5f8a8c9422ca1d727a830c94d23",
              "IPY_MODEL_33987886f5714c00b172819cbd640650"
            ],
            "layout": "IPY_MODEL_44db444b888d47dd9a583457738fda7c"
          }
        },
        "e1749af1822647e09659a00318099947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ec680839e0e44f787e52e23fb647d79",
            "placeholder": "​",
            "style": "IPY_MODEL_3ccd98504d3f49f3827aaeb14d7b207a",
            "value": "model.safetensors: 100%"
          }
        },
        "ccafe5f8a8c9422ca1d727a830c94d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d08649f71ad4c529bd6ac3b8f2f4e12",
            "max": 102469840,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98da238b89e34b0bb87453a66533e851",
            "value": 102469840
          }
        },
        "33987886f5714c00b172819cbd640650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_653481de7d044e81b93fd23878ffaa3b",
            "placeholder": "​",
            "style": "IPY_MODEL_eee15c6b2b1e48a1b75f5ce9504b6a45",
            "value": " 102M/102M [00:00&lt;00:00, 176MB/s]"
          }
        },
        "44db444b888d47dd9a583457738fda7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ec680839e0e44f787e52e23fb647d79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ccd98504d3f49f3827aaeb14d7b207a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d08649f71ad4c529bd6ac3b8f2f4e12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98da238b89e34b0bb87453a66533e851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "653481de7d044e81b93fd23878ffaa3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eee15c6b2b1e48a1b75f5ce9504b6a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Flower Recognition with Transfer Learning**\n",
        "\n",
        "# Summary\n",
        "In this notebook we develop an AI-based system to automatically classify two flower species — **daisy** and **dandelion** — using transfer learning on ResNet-50 via the **timm** library.  \n",
        "We optimize for **macro F1-score** on a held-out test set, reaching ≈0.89 F1 and 0.90 accuracy.\n",
        "\n",
        "The code below take inspiration from a project of mine for DL with PyTorch ([here](https://colab.research.google.com/drive/1tp_s8WTgODxTt7DAPiNTyVMlEYY6GgsA) the link)"
      ],
      "metadata": {
        "id": "6Ws2hEqw-swV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction & Methodology\n",
        "**Objective:**  \n",
        "Automatically recognize **daisy** and **dandelion** from RGB images to support GreenTech Solutions in crop health monitoring.\n",
        "\n",
        "**Approach:**  \n",
        "- **Transfer learning** with ImageNet-pretrained ResNet-50 via `timm.create_model`.\n",
        "- **Data augmentations** (RandAugment + Random Erasing) from `timm.data.create_transform` to improve robustness.\n",
        "- **Two-phase fine-tuning**:  \n",
        "  1. Freeze backbone, train only classifier head;\n",
        "  2. Unfreeze `layer4`, fine-tune last block at lower LR.\n",
        "- **Evaluation metric:** macro F1-score + accuracy on validation and test.\n",
        "\n",
        "**Key design decisions:**  \n",
        "- ResNet-50: trade-off between accuracy and compute cost.  \n",
        "- `rand-m9-mstd0.5`: strong augmentations for color/shape variation.  \n",
        "- `ReduceLROnPlateau` scheduler to adapt LR on validation loss.  \n",
        "- `is_valid_file` filter to skip corrupted or macOS “._” files."
      ],
      "metadata": {
        "id": "fOC0Jq-8_eMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirement"
      ],
      "metadata": {
        "id": "91QGResgbSI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models #we could also try transforms\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "\n",
        "import timm\n",
        "from timm.data import create_transform\n",
        "\n",
        "import copy\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "om5ekH1MbY-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download & Extract dataset"
      ],
      "metadata": {
        "id": "-HliagjFcQ8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract_dataset(url: str, dest_folder: str):\n",
        "\n",
        "    \"\"\"\n",
        "    Download and extract a tar.gz archive from a URL.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL pointing to the .tar.gz dataset.\n",
        "        dest_folder (str): Local folder where files will be extracted.\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "    # Skip download if already extracted\n",
        "    if all(os.path.isdir(os.path.join(dest_folder, split)) for split in ['test','train','valid']):\n",
        "        print(\"Dataset already exists, skip download.\")\n",
        "        return\n",
        "\n",
        "    archive_path = os.path.join(dest_folder, 'flowers_dataset.tar.gz')\n",
        "    print(f\"Downloading dataset from {url}...\")\n",
        "    urllib.request.urlretrieve(url, archive_path)\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "    print(\"Extracting files...\")\n",
        "    with tarfile.open(archive_path, 'r:gz') as tar:\n",
        "        tar.extractall(path=dest_folder)\n",
        "    print(\"Extraction complete.\")\n",
        "\n",
        "    os.remove(archive_path)\n",
        "    print(\"Cleaned up archive.\")"
      ],
      "metadata": {
        "id": "7wmhW7X0cGHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base URL\n",
        "DATASET_URL = \"https://proai-datasets.s3.eu-west-3.amazonaws.com/progetto-finale-flowes.tar.gz\"\n",
        "\n",
        "DATASET_FOLDER = \"/content/data/flowers\"\n",
        "\n",
        "# Download and extract (skip if already done)\n",
        "download_and_extract_dataset(DATASET_URL, DATASET_FOLDER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFhCWZtEc38Y",
        "outputId": "68cc2699-70a1-4d1d-a02e-a5572debf659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset from https://proai-datasets.s3.eu-west-3.amazonaws.com/progetto-finale-flowes.tar.gz...\n",
            "Download complete.\n",
            "Extracting files...\n",
            "Extraction complete.\n",
            "Cleaned up archive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform, Dataset & Dataloader"
      ],
      "metadata": {
        "id": "Fior2rjGjDDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Define timm-based transforms\n",
        "\n",
        "def get_timm_transforms(input_size: int = 224):\n",
        "\n",
        "    \"\"\"\n",
        "    Create train/valid/test transforms using timm's create_transform.\n",
        "    \"\"\"\n",
        "\n",
        "    # ImageNet normalization\n",
        "    normalize = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "\n",
        "    # Training transform: random crop, RandAugment, Random Erasing, normalization\n",
        "    train_tf = create_transform(\n",
        "        input_size=input_size,\n",
        "        is_training=True,\n",
        "        # RandAugment policy for strong color and geometric variations\n",
        "        auto_augment='rand-m9-mstd0.5-inc1',\n",
        "        # Random Erasing to simulate occlusions and improve generalization\n",
        "        re_prob=0.25, # random erase probability\n",
        "        re_mode='pixel',\n",
        "        re_count=1,\n",
        "        mean=normalize[0],\n",
        "        std=normalize[1]\n",
        "    )\n",
        "\n",
        "    # Validation/test transform: center crop + normalization\n",
        "    val_tf = create_transform(\n",
        "        input_size=input_size,\n",
        "        is_training=False,\n",
        "        mean=normalize[0],\n",
        "        std=normalize[1]\n",
        "    )\n",
        "\n",
        "    return {'train': train_tf, 'valid': val_tf, 'test': val_tf}"
      ],
      "metadata": {
        "id": "Yz8nX66Fn58X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Define valid-file filter to skip non-image or macOS “._” files\n",
        "\n",
        "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
        "\n",
        "def is_valid_file(path: str) -> bool:\n",
        "\n",
        "    \"\"\"\n",
        "    Return True only if:\n",
        "      - the file has a valid image extension\n",
        "      - the filename does NOT start with '._'\n",
        "    \"\"\"\n",
        "\n",
        "    fname = os.path.basename(path)\n",
        "    return fname.lower().endswith(IMG_EXTENSIONS) and not fname.startswith('._')"
      ],
      "metadata": {
        "id": "AC1SN3wBohxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Prepare transforms\n",
        "INPUT_SIZE = 224\n",
        "transforms_dict = get_timm_transforms(input_size=INPUT_SIZE)\n",
        "\n",
        "# 4) Create ImageFolder datasets with is_valid_file filter\n",
        "DATASET_FOLDER = \"/content/data/flowers/progetto-finale-flowes\"\n",
        "datasets_dict = {}\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    split_dir = os.path.join(DATASET_FOLDER, split)\n",
        "    datasets_dict[split] = datasets.ImageFolder(\n",
        "        root=split_dir,\n",
        "        transform=transforms_dict[split],\n",
        "        is_valid_file=is_valid_file\n",
        "    )\n",
        "\n",
        "# 5) Wrap datasets in DataLoaders\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "dataloaders = {}\n",
        "for split, ds in datasets_dict.items():\n",
        "    dataloaders[split] = DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=(split == 'train'),\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "# 6) Inspect classes\n",
        "print(\"Classes:\", datasets_dict['train'].classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc7KkOs2o5th",
        "outputId": "2aadbd3b-26f9-4d9d-dcc8-73c9add5595a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['daisy', 'dandelion']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "9bJndV2io-6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize"
      ],
      "metadata": {
        "id": "dF-yRehw6Nz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model & Hyperparameters**\n",
        "\n",
        "- **Architecture:** ResNet-50 from `timm`  \n",
        "- **Batch size:** 32  \n",
        "- **Learning rates:** 1e-4 (phase1), 1e-5 (phase2)  \n",
        "- **Weight decay:** 1e-5  \n",
        "- **Epochs:** 5 (head only) + 5 (fine-tuning)  \n",
        "- **Scheduler:** `ReduceLROnPlateau` on validation loss (factor=0.1, patience=3)  \n",
        "- **Metrics:** macro F1-score primary, also track accuracy"
      ],
      "metadata": {
        "id": "OFPVkUntAP4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_timm_model(model_name: str,\n",
        "                          num_classes: int,\n",
        "                          feature_extract: bool = True,\n",
        "                          pretrained: bool = True):\n",
        "\n",
        "    \"\"\"\n",
        "    Create a timm model, replace its classifier head, and optionally freeze backbone.\n",
        "    \"\"\"\n",
        "\n",
        "    model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
        "    if feature_extract:\n",
        "        # 1) Freeze all\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        # 2) Unfreeze only the classifier head\n",
        "        if hasattr(model, 'fc'):           # Typical ResNet\n",
        "            for p in model.fc.parameters():\n",
        "                p.requires_grad = True\n",
        "        elif hasattr(model, 'classifier'):\n",
        "            for p in model.classifier.parameters():\n",
        "                p.requires_grad = True\n",
        "        elif hasattr(model, 'head'):\n",
        "            for p in model.head.parameters():\n",
        "                p.requires_grad = True\n",
        "        else:\n",
        "            raise ValueError(\"Cannot find fc/classifier/head in model to unfreeze.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "u9vGdbwWo8P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train one epoch"
      ],
      "metadata": {
        "id": "p-X-KLXY6h4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "\n",
        "    \"\"\"\n",
        "    Train the model for one epoch; return:\n",
        "      - epoch_loss: average loss over all samples\n",
        "      - epoch_f1:   macro F1-score over the epoch\n",
        "      - epoch_acc: accuracy over the epoch\n",
        "    \"\"\"\n",
        "\n",
        "    model.train() # Switch to training mode (enables dropout, batchnorm updates)\n",
        "    running_loss = 0.0\n",
        "    all_preds, all_labels = [], []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate over the DataLoader batches\n",
        "    for inputs, labels in tqdm(loader, desc=\"train\"):\n",
        "        # Move inputs and labels to the computation device (CPU or GPU)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() # Reset gradients from previous step\n",
        "        outputs = model(inputs) # Forward pass: compute logits\n",
        "        loss = criterion(outputs, labels) # Compute classification loss\n",
        "        loss.backward() # Backward pass: compute gradients\n",
        "        optimizer.step() # Update model parameters\n",
        "\n",
        "        # Accumulate batch loss (loss * number of samples)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        # Compute predictions: index of max logit per sample\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "        # Update correct prediction count for accuracy\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    # Average loss over entire epoch\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    # Compute macro F1-score across all classes\n",
        "    epoch_f1   = f1_score(all_labels, all_preds, average='macro')\n",
        "    # Compute accuracy as total correct / total samples\n",
        "    epoch_acc  = correct / total\n",
        "    return epoch_loss, epoch_f1, epoch_acc"
      ],
      "metadata": {
        "id": "VRvH5oTcpaxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate one epoch"
      ],
      "metadata": {
        "id": "FQz8NWla6pMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() # Disable gradient computation for efficiency\n",
        "def validate_one_epoch(model, loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Validate the model for one epoch; return:\n",
        "      - epoch_loss: average loss over all samples\n",
        "      - epoch_f1:   macro F1-score over the epoch\n",
        "      - epoch_acc:  accuracy over the epoch\n",
        "      - all_labels: list of true labels (for detailed reports)\n",
        "      - all_preds:  list of predicted labels\n",
        "    \"\"\"\n",
        "    model.eval() # Switch to evaluation mode (disable dropout, use running stats for batchnorm)\n",
        "    running_loss = 0.0\n",
        "    all_preds, all_labels = [], []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate over the validation DataLoader batches\n",
        "    for inputs, labels in tqdm(loader, desc=\"valid\"):\n",
        "        # Move inputs and labels to the computation device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        # Compute classification loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Accumulate batch loss (loss * number of samples)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        # Compute predictions: index of max logit per sample\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "        # Update correct prediction count for accuracy\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_f1   = f1_score(all_labels, all_preds, average='macro')\n",
        "    epoch_acc  = correct / total\n",
        "    return epoch_loss, epoch_f1, epoch_acc, all_labels, all_preds"
      ],
      "metadata": {
        "id": "ecVr1NrU6pkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit & Validate\n",
        "\n",
        "We perform **two-phase training**:\n",
        "1. **Phase 1**: only train the new head (fast convergence).  \n",
        "2. **Phase 2**: unfreeze last block (`layer4`) and fine-tune at lower LR.\n",
        "\n",
        "At each epoch we record **F1-macro** and **accuracy** on both train and validation sets.\n"
      ],
      "metadata": {
        "id": "9TdmM6vk6xcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, dataloaders, device, criterion,\n",
        "        init_lr=1e-4, ft_lr=1e-5, weight_decay=1e-5,\n",
        "        init_epochs=5, ft_epochs=5, unfreeze_layer='layer4'):\n",
        "\n",
        "    \"\"\"\n",
        "    Two-phase training:\n",
        "      Phase 1: train head only.\n",
        "      Phase 2: unfreeze last block and fine-tune.\n",
        "    \"\"\"\n",
        "\n",
        "    # save initial weights to restore best model later\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_f1 = 0.0 # track best validation F1-score\n",
        "\n",
        "    # Phase 1: head only\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                           lr=init_lr, weight_decay=weight_decay)\n",
        "    # scheduler reduces LR when validation loss plateaus\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                     mode='min',\n",
        "                                                     factor=0.1,\n",
        "                                                     patience=3)\n",
        "    for epoch in range(1, init_epochs+1):\n",
        "        print(f\"\\n[Phase1] Epoch {epoch}/{init_epochs}\")\n",
        "        # train and validate one epoch\n",
        "        tr_loss, tr_f1, tr_acc = train_one_epoch(model, dataloaders['train'], criterion, optimizer, device)\n",
        "        val_loss, val_f1, val_acc, _, _ = validate_one_epoch(model, dataloaders['valid'], criterion, device)\n",
        "        # update scheduler based on validation loss\n",
        "        scheduler.step(val_loss)\n",
        "        print(\n",
        "            f\" train F1: {tr_f1:.4f}, acc: {tr_acc:.4f} | \"\n",
        "            f\"valid F1: {val_f1:.4f}, acc: {val_acc:.4f} | \"\n",
        "            f\"lr={scheduler.get_last_lr()[0]:.1e}\"\n",
        "        )\n",
        "        # if validation F1 improves, save model weights\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # Phase 2: unfreeze last block\n",
        "    for name, param in model.named_parameters():\n",
        "        if name.startswith(unfreeze_layer):\n",
        "            param.requires_grad = True\n",
        "    print(f\"\\nUnfroze {unfreeze_layer}, fine-tuning last block...\")\n",
        "    # restore weights from best validation epoch before fine-tuning\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    # reconfigure optimizer & scheduler to include the newly unfrozen parameters\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                           lr=ft_lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                     mode='min',\n",
        "                                                     factor=0.1,\n",
        "                                                     patience=3)\n",
        "    for epoch in range(1, ft_epochs+1):\n",
        "        print(f\"\\n[Phase2] Epoch {epoch}/{ft_epochs}\")\n",
        "        tr_loss, tr_f1, tr_acc = train_one_epoch(model, dataloaders['train'], criterion, optimizer, device)\n",
        "        val_loss, val_f1, val_acc, _, _ = validate_one_epoch(model, dataloaders['valid'], criterion, device)\n",
        "        scheduler.step(val_loss)\n",
        "        print(\n",
        "            f\" train F1: {tr_f1:.4f}, acc: {tr_acc:.4f} | \"\n",
        "            f\"valid F1: {val_f1:.4f}, acc: {val_acc:.4f} | \"\n",
        "            f\"lr={scheduler.get_last_lr()[0]:.1e}\"\n",
        "        )\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # load best overall weights before returning the model\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "b8HFadB36yK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select device: prefer GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Number of output classes inferred from the training dataset\n",
        "num_classes = len(datasets_dict['train'].classes)\n",
        "\n",
        "# Initialize the ResNet-50 model with timm:\n",
        "# - feature_extract=True freezes the backbone initially\n",
        "# - pretrained=True loads ImageNet weights\n",
        "model = initialize_timm_model('resnet50', num_classes,\n",
        "                              feature_extract=True, pretrained=True).to(device) # move model to the chosen device\n",
        "\n",
        "# Define the classification loss function (cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Run the two-phase training routine:\n",
        "# Phase 1: train only the new classifier head (fast initial convergence)\n",
        "# Phase 2: unfreeze 'layer4' and fine-tune at a lower learning rate\n",
        "model = fit(\n",
        "    model,\n",
        "    dataloaders,\n",
        "    device,\n",
        "    criterion,\n",
        "    init_lr=1e-4,       # learning rate for head training\n",
        "    ft_lr=1e-5,         # learning rate for fine-tuning last block\n",
        "    weight_decay=1e-5,  # L2 regularization strength\n",
        "    init_epochs=5,      # number of epochs for phase 1\n",
        "    ft_epochs=5,        # number of epochs for phase 2\n",
        "    unfreeze_layer='layer4'  # which block to unfreeze in phase 2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c67e10118d3b4a329f2967f07a7b0c4c",
            "e1749af1822647e09659a00318099947",
            "ccafe5f8a8c9422ca1d727a830c94d23",
            "33987886f5714c00b172819cbd640650",
            "44db444b888d47dd9a583457738fda7c",
            "5ec680839e0e44f787e52e23fb647d79",
            "3ccd98504d3f49f3827aaeb14d7b207a",
            "5d08649f71ad4c529bd6ac3b8f2f4e12",
            "98da238b89e34b0bb87453a66533e851",
            "653481de7d044e81b93fd23878ffaa3b",
            "eee15c6b2b1e48a1b75f5ce9504b6a45"
          ]
        },
        "id": "PQ_ARQrz7SDu",
        "outputId": "94b3a58c-de9b-4447-c346-7d651bef2ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c67e10118d3b4a329f2967f07a7b0c4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Phase1] Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 40/40 [00:07<00:00,  5.21it/s]\n",
            "valid: 100%|██████████| 12/12 [00:01<00:00,  6.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " train F1: 0.5927, acc: 0.6298 | valid F1: 0.5157, acc: 0.6264 | lr=1.0e-04\n",
            "\n",
            "[Phase1] Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 40/40 [00:07<00:00,  5.42it/s]\n",
            "valid: 100%|██████████| 12/12 [00:01<00:00,  6.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " train F1: 0.5038, acc: 0.6400 | valid F1: 0.6630, acc: 0.7143 | lr=1.0e-04\n",
            "\n",
            "[Phase1] Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 40/40 [00:07<00:00,  5.51it/s]\n",
            "valid: 100%|██████████| 12/12 [00:01<00:00,  6.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " train F1: 0.6664, acc: 0.7263 | valid F1: 0.8572, acc: 0.8654 | lr=1.0e-04\n",
            "\n",
            "[Phase1] Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 40/40 [00:06<00:00,  6.27it/s]\n",
            "valid: 100%|██████████| 12/12 [00:02<00:00,  4.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " train F1: 0.7429, acc: 0.7796 | valid F1: 0.8938, acc: 0.8984 | lr=1.0e-04\n",
            "\n",
            "[Phase1] Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 40/40 [00:06<00:00,  6.02it/s]\n",
            "valid: 100%|██████████| 12/12 [00:01<00:00,  6.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " train F1: 0.7552, acc: 0.7867 | valid F1: 0.9089, acc: 0.9121 | lr=1.0e-04\n",
            "\n",
            "Unfroze layer4, fine-tuning last block...\n",
            "\n",
            "[Phase2] Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 40/40 [00:08<00:00,  4.80it/s]\n",
            "valid: 100%|██████████| 12/12 [00:01<00:00,  6.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " train F1: 0.7882, acc: 0.8102 | valid F1: 0.9235, acc: 0.9258 | lr=1.0e-05\n",
            "\n",
            "[Phase2] Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 40/40 [00:07<00:00,  5.08it/s]\n",
            "valid: 100%|██████████| 12/12 [00:01<00:00,  6.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " train F1: 0.8032, acc: 0.8220 | valid F1: 0.9267, acc: 0.9286 | lr=1.0e-05\n",
            "\n",
            "[Phase2] Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 40/40 [00:06<00:00,  5.84it/s]\n",
            "valid: 100%|██████████| 12/12 [00:02<00:00,  4.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " train F1: 0.8130, acc: 0.8306 | valid F1: 0.9295, acc: 0.9313 | lr=1.0e-05\n",
            "\n",
            "[Phase2] Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 40/40 [00:07<00:00,  5.70it/s]\n",
            "valid: 100%|██████████| 12/12 [00:01<00:00,  6.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " train F1: 0.8226, acc: 0.8384 | valid F1: 0.9323, acc: 0.9341 | lr=1.0e-05\n",
            "\n",
            "[Phase2] Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train: 100%|██████████| 40/40 [00:07<00:00,  5.05it/s]\n",
            "valid: 100%|██████████| 12/12 [00:01<00:00,  6.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " train F1: 0.8478, acc: 0.8604 | valid F1: 0.9354, acc: 0.9368 | lr=1.0e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**  \n",
        "- The two‐phase schedule allows the classifier head to learn robustly first, then refines deeper representations in `layer4`;\n",
        "- Validation F1/macro and accuracy consistently improve in both phases, peaking in Phase 2;\n",
        "- No signs of overfitting: training and validation curves move in tandem, and the LR scheduler effectively adapts to plateauing;\n",
        "- Final validation performance (~0.94 F1/acc) confirms the effectiveness of strong augmentations and two-phase fine-tuning."
      ],
      "metadata": {
        "id": "mxLsKZTkEpgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final evaluation on test set"
      ],
      "metadata": {
        "id": "puJ790zn64HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_f1, test_acc, y_true, y_pred = validate_one_epoch(\n",
        "    model, dataloaders['test'], criterion, device)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Test F1-macro: {test_f1:.4f}, Test acc: {test_acc:.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_true, y_pred,\n",
        "                                                        target_names=datasets_dict['train'].classes,\n",
        "                                                        digits=4, zero_division=0))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIWcKi_Q642p",
        "outputId": "991a30e6-4ce2-4bba-d93f-8cb449deb7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid: 100%|██████████| 6/6 [00:02<00:00,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 0.4357, Test F1-macro: 0.8943, Test acc: 0.9011\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       daisy     1.0000    0.7662    0.8676        77\n",
            "   dandelion     0.8537    1.0000    0.9211       105\n",
            "\n",
            "    accuracy                         0.9011       182\n",
            "   macro avg     0.9268    0.8831    0.8943       182\n",
            "weighted avg     0.9156    0.9011    0.8985       182\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 59  18]\n",
            " [  0 105]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Set Performance & Future Improvements\n",
        "\n",
        "**Test Set Results:**  \n",
        "- **Loss:** 0.4357  \n",
        "- **F1-macro:** 0.8943  \n",
        "- **Accuracy:** 0.9011  \n",
        "\n",
        "**Classification Report:**  \n",
        "- **Daisy:** Precision 1.00, Recall 0.77 → The model makes no false positives on daisies (very precise), but misses about 23% of actual daisies (lower recall).  \n",
        "- **Dandelion:** Precision 0.85, Recall 1.00 → All true dandelions are correctly identified (perfect recall), but about 15% of its positive predictions are actually daisies (some false positives).\n",
        "\n",
        "**Confusion Matrix:**\n",
        "- 59 true daisies, 18 daisies misclassified as dandelions  \n",
        "- 105 true dandelions, 0 misclassified  \n",
        "\n",
        "By addressing class imbalance and exploring stronger augmentation and ensemble strategies, we can push both recall and precision higher, aiming for even better F1 and accuracy on daisies and dandelions alike."
      ],
      "metadata": {
        "id": "F-BpwM0DFFBY"
      }
    }
  ]
}
